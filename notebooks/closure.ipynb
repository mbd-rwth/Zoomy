{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from library.misc.io import load_fields_from_hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_input = \"/home/ingo/Git/SMM/shallow-moments-simulation/openfoam_data/channelflow_coarse/closure_dataset_input.hdf5\"\n",
    "path_data_output = \"/home/ingo/Git/SMM/shallow-moments-simulation/openfoam_data/channelflow_coarse/closure_dataset_output.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multi sindy\n",
    "Problem: not working with only two snapshots in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path_data_input, \"r\") as f:\n",
    "    n_iterations = len(f.keys())\n",
    "    Q, Qaux, time = load_fields_from_hdf5(path_data_input, 0)\n",
    "    n_elements = Q.shape[0]\n",
    "    n_fields = Q.shape[1]\n",
    "\n",
    "input = np.zeros((n_iterations, n_elements, n_fields))\n",
    "output = np.zeros((n_iterations, n_elements, n_fields))\n",
    "dts = np.zeros((n_iterations, n_elements)) \n",
    "for i in range(n_iterations):\n",
    "    output[i, :, :] = deepcopy(Q)\n",
    "    Q, Qaux, time = load_fields_from_hdf5(path_data_input, i)\n",
    "    input[i, :, :] = deepcopy(Q)\n",
    "    Q, Qaux, dt = load_fields_from_hdf5(path_data_output, i)\n",
    "    dts[i, :] = dt\n",
    "input = input.reshape(n_iterations*n_elements, n_fields)\n",
    "output = output.reshape(n_iterations*n_elements, n_fields)\n",
    "dts = dts.reshape(n_iterations*n_elements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go to primitive variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, n_fields):\n",
    "    input[:,i] = input[:,i] / input[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NAN entries in output: 0/29400\n",
      "number of inf entries in output: 0/29400\n",
      "number of non-zero entries in output: 23217/29400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"number of NAN entries in output: {np.sum(np.isnan(output) > 0)}/{output.shape[0]*output.shape[1]}\")\n",
    "print(f\"number of inf entries in output: {np.sum(np.isinf(output) > 0)}/{output.shape[0]*output.shape[1]}\")\n",
    "print(f\"number of non-zero entries in output: {np.sum(np.abs(output) > 0)}/{output.shape[0]*output.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [Multiple trajectories](https://pysindy.readthedocs.io/en/latest/examples/1_feature_overview/example.html#Single-trajectory,-pass-in-collection-times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHEATING\n",
    "because we need at least 3 time steps (maybe a custom fd model helps?), I assume a linear relation in time anf sample the output as list [IC, IC+0.5*dQ, IC_dQ]. This is clearly an assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "T_data = []\n",
    "for i in range(n_iterations*n_elements):\n",
    "    x_data.append(np.stack((input[i, :], input[i,:] + 0.5 *output[i, :], input[i,:] + 1.0 *output[i, :])))\n",
    "    T_data.append( np.array([0, 0.5*dts[i], 1.0 * dts[i]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.linspace(0, len(x_data)-1, len(x_data), dtype=int)\n",
    "from sklearn.model_selection import train_test_split\n",
    "I_train, I_test = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "x_test = [x_data[i] for i in I_test]\n",
    "T_test = [T_data[i] for i in I_test]\n",
    "x_train = [x_data[i] for i in I_train]\n",
    "T_train = [T_data[i] for i in I_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysindy as ps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(h)' = 0.276 f0(h,u,u1,v,v1) + 0.690 f1(h,u,u1,v,v1)\n",
      "(u)' = 3.185 f0(h,u,u1,v,v1)\n",
      "(u1)' = 1.657 f0(h,u,u1,v,v1) + -2.990 f1(h,u,u1,v,v1)\n",
      "(v)' = 0.305 f1(h,u,u1,v,v1)\n",
      "(v1)' = -0.293 f1(h,u,u1,v,v1)\n"
     ]
    }
   ],
   "source": [
    "## Create data\n",
    "# library_poly = ps.PolynomialLibrary(degree=2, include_bias=False)\n",
    "library_unsym = ps.CustomLibrary(\n",
    "    library_functions=[lambda h, u, u1, v, v1: abs(u), lambda h, u, u2, v, v1: abs(v)])\n",
    "    # library_functions=[lambda h, u, v : abs(u), lambda h, u, v: abs(u)])\n",
    "combined_library = library_unsym\n",
    "# feature_names = ['h', 'u', 'v']\n",
    "feature_names = ['h', 'u', 'u1', 'v', 'v1']\n",
    "model_unsym = ps.SINDy(feature_library = combined_library, feature_names = feature_names)\n",
    "# model = ps.SINDy(feature_library = library_poly)\n",
    "model_unsym.fit(x_train, t=T_train, multiple_trajectories=True, unbias=True)\n",
    "model_unsym.print()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(model, x_test, T_test):\n",
    "    # considered_fields = [1, 2]\n",
    "    considered_fields = [1, 3]\n",
    "    error = 0.\n",
    "    mean = 0.\n",
    "    n_tests = len(x_test)\n",
    "    for i in range(n_tests):\n",
    "        x0 = x_test[i][0]\n",
    "        y = x_test[i][:]\n",
    "        time = T_test[i][:]\n",
    "        y_sim = model.simulate(x0, time)\n",
    "        error += np.linalg.norm((y-y_sim)[-1, considered_fields])\n",
    "        mean += np.linalg.norm((y[-1,considered_fields]))\n",
    "    error = error/n_tests\n",
    "    mean = mean/n_tests\n",
    "    print(f'absolute L2 error {error}')\n",
    "    print(f'relative L2 error {error/mean}')\n",
    "    print(f'mean {mean}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute L2 error 0.005065342944989903\n",
      "relative L2 error 0.04224022334858655\n",
      "mean 0.11991752276469438\n",
      "None\n",
      "absolute L2 error 0.0057794632401142335\n",
      "relative L2 error 0.045803889639148186\n",
      "mean 0.12617843780617655\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(compute_error(model_unsym, x_test , T_test))\n",
    "print(compute_error(model_unsym, x_train, T_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraint problem for level = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 6 into shape (0,20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# feature_names = ['h', 'u', 'u1', 'v', 'v1']\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model_sym \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mSINDy(feature_library \u001b[38;5;241m=\u001b[39m combined_library, feature_names \u001b[38;5;241m=\u001b[39m feature_names, optimizer\u001b[38;5;241m=\u001b[39mopt)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel_sym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiple_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m model_sym\u001b[38;5;241m.\u001b[39mprint()\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/pysindy/pysindy.py:414\u001b[0m, in \u001b[0;36mSINDy.fit\u001b[0;34m(self, x, t, x_dot, u, multiple_trajectories, unbias, quiet, ensemble, library_ensemble, replace, n_candidates_to_drop, n_subset, n_models, ensemble_aggregator)\u001b[0m\n\u001b[1;32m    412\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(action, category\u001b[38;5;241m=\u001b[39mLinAlgWarning)\n\u001b[1;32m    413\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(action, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# New version of sklearn changes attribute name\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(__version__[:\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 475\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/pysindy/optimizers/sindy_optimizer.py:60\u001b[0m, in \u001b[0;36mSINDyOptimizer.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m     55\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m drop_nan_samples(\n\u001b[1;32m     56\u001b[0m         AxesArray(x, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max_coord\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}),\n\u001b[1;32m     57\u001b[0m         AxesArray(y, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max_coord\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}),\n\u001b[1;32m     58\u001b[0m     )\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer has no attribute coef_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/pysindy/optimizers/base.py:178\u001b[0m, in \u001b[0;36mBaseOptimizer.fit\u001b[0;34m(self, x_, y, sample_weight, **reduce_kws)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_]\n\u001b[1;32m    176\u001b[0m x_normed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x_normed)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_normed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreduce_kws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mind_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-14\u001b[39m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Rescale coefficients to original units\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/pysindy/optimizers/constrained_sr3.py:393\u001b[0m, in \u001b[0;36mConstrainedSR3._reduce\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_trimming_ \u001b[38;5;241m=\u001b[39m [trimming_array]\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_constraints \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraint_order\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraint_lhs \u001b[38;5;241m=\u001b[39m \u001b[43mreorder_constraints\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstraint_lhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# Precompute some objects for upcoming least-squares solves.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Assumes that self.nu is fixed throughout optimization procedure.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m H \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x\u001b[38;5;241m.\u001b[39mT, x) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mfull(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu))\n",
      "File \u001b[0;32m~/Git/SMM/shallow-moments-simulation/.venv/lib/python3.9/site-packages/pysindy/utils/base.py:147\u001b[0m, in \u001b[0;36mreorder_constraints\u001b[0;34m(c, n_features, output_order)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ret\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 147\u001b[0m         ret[i] \u001b[38;5;241m=\u001b[39m \u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ret\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 6 into shape (0,20)"
     ]
    }
   ],
   "source": [
    "## Create data\n",
    "n_constraints = 5\n",
    "n_targets = x_train[0].shape[1]\n",
    "n_features = 2\n",
    "n_eqns = 3\n",
    "constraint_rhs = np.zeros(n_constraints)\n",
    "constraint_lhs = np.zeros((n_constraints, n_eqns*n_features))\n",
    "# coefficients of Abs(u) == Abs(v) in u'\n",
    "constraint_lhs[0, n_features + 0] = 1\n",
    "constraint_lhs[0, n_features + 1] = -1\n",
    "# coefficients of Abs(u) == Abs(v) in v'\n",
    "constraint_lhs[1, 2*n_features + 0] = 1\n",
    "constraint_lhs[1, 2*n_features + 1] = -1\n",
    "# coefs for u' and v' are the same\n",
    "constraint_lhs[2, 1*n_features + 0] = 1\n",
    "constraint_lhs[2, 2*n_features + 0] = -1\n",
    "# h' is not affcted from Abs(u)\n",
    "constraint_lhs[3, 0] = 1\n",
    "# h' is not affcted from Abs(v)\n",
    "constraint_lhs[4, 1] = 1\n",
    "\n",
    "opt = ps.ConstrainedSR3(constraint_lhs=constraint_lhs, constraint_rhs=constraint_rhs, threshold=0.5, thresholder='l1')\n",
    "# opt = ps.SR3(threshold=0.5, thresholder='l1')\n",
    "library_sym = ps.CustomLibrary(\n",
    "    library_functions=[lambda x, y, z: abs(y), lambda x, y, z: abs(z)])\n",
    "    # library_functions=[lambda h, u, u1, v, v1: abs(u), lambda h, u, u2, v, v1: abs(v)])\n",
    "combined_library = library_sym\n",
    "feature_names = ['h', 'u', 'v']\n",
    "# feature_names = ['h', 'u', 'u1', 'v', 'v1']\n",
    "model_sym = ps.SINDy(feature_library = combined_library, feature_names = feature_names, optimizer=opt)\n",
    "\n",
    "\n",
    "model_sym.fit(x_train, t=T_train, multiple_trajectories=True)\n",
    "model_sym.print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute L2 error 0.004802322289611073\n",
      "relative L2 error 0.04004687704426924\n",
      "mean 0.11991752276469438\n",
      "None\n",
      "absolute L2 error 0.005634422971122236\n",
      "relative L2 error 0.04465440426340756\n",
      "mean 0.12617843780617655\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(compute_error(model_sym, x_test , T_test))\n",
    "print(compute_error(model_sym, x_train, T_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraint problem for level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(h)' = 0.000\n",
      "(u)' = 1.092 f0(h,u,u1,v,v1) + -0.144 f1(h,u,u1,v,v1) + 1.092 f2(h,u,u1,v,v1) + -0.144 f3(h,u,u1,v,v1)\n",
      "(u1)' = 0.281 f1(h,u,u1,v,v1) + 0.281 f3(h,u,u1,v,v1)\n",
      "(v)' = 1.092 f0(h,u,u1,v,v1) + -0.144 f1(h,u,u1,v,v1) + 1.092 f2(h,u,u1,v,v1) + -0.144 f3(h,u,u1,v,v1)\n",
      "(v1)' = 0.281 f1(h,u,u1,v,v1) + 0.281 f3(h,u,u1,v,v1)\n"
     ]
    }
   ],
   "source": [
    "## Create data\n",
    "n_constraints = 16\n",
    "n_targets = x_train[0].shape[1]\n",
    "n_features = 4\n",
    "n_eqns = 5\n",
    "constraint_rhs = np.zeros(n_constraints)\n",
    "constraint_lhs = np.zeros((n_constraints, n_eqns*n_features))\n",
    "# coefficients of Abs(u) == Abs(v) in u'\n",
    "constraint_lhs[0, n_features + 0] = 1\n",
    "constraint_lhs[0, n_features + 2] = -1\n",
    "# coefficients of Abs(u) == Abs(v) in v'\n",
    "constraint_lhs[1, 3*n_features + 0] = 1\n",
    "constraint_lhs[1, 3*n_features + 2] = -1\n",
    "# coefs for u' and v' are the same\n",
    "constraint_lhs[2, 1*n_features + 0] = 1\n",
    "constraint_lhs[2, 3*n_features + 0] = -1\n",
    "\n",
    "# coefficients of Abs(u1) == Abs(v1) in u'\n",
    "constraint_lhs[3, n_features + 1] = 1\n",
    "constraint_lhs[3, n_features + 3] = -1\n",
    "# coefficients of Abs(u1) == Abs(v1) in v'\n",
    "constraint_lhs[4, 3*n_features + 1] = 1\n",
    "constraint_lhs[4, 3*n_features + 3] = -1\n",
    "# coefs for u1' and v1' are the same\n",
    "constraint_lhs[5, 1*n_features + 1] = 1\n",
    "constraint_lhs[5, 3*n_features + 1] = -1\n",
    "\n",
    "# coefficients of Abs(u) == Abs(v) in u1'\n",
    "constraint_lhs[6, 2*n_features + 0] = 1\n",
    "constraint_lhs[6, 2*n_features + 2] = -1\n",
    "# coefficients of Abs(u) == Abs(v) in v1'\n",
    "constraint_lhs[7, 4*n_features + 0] = 1\n",
    "constraint_lhs[7, 4*n_features + 2] = -1\n",
    "# coefs for u' and v' are the same\n",
    "constraint_lhs[8, 2*n_features + 0] = 1\n",
    "constraint_lhs[8, 4*n_features + 0] = -1\n",
    " \n",
    "# coefficients of Abs(u1) == Abs(v1) in u1'\n",
    "constraint_lhs[9, 2*n_features + 1] = 1\n",
    "constraint_lhs[9, 2*n_features + 3] = -1\n",
    "# coefficients of Abs(u1) == Abs(v1) in v1'\n",
    "constraint_lhs[10, 4*n_features + 1] = 1\n",
    "constraint_lhs[10, 4*n_features + 3] = -1\n",
    "# coefs for u1' and v1' are the same\n",
    "constraint_lhs[11, 2*n_features + 1] = 1\n",
    "constraint_lhs[11, 4*n_features + 1] = -1\n",
    "\n",
    "# h' is not affcted from Abs(u)\n",
    "constraint_lhs[12, 0] = 1\n",
    "# h' is not affcted from Abs(v)\n",
    "constraint_lhs[13, 1] = 1\n",
    "# h' is not affcted from Abs(u1)\n",
    "constraint_lhs[14, 2] = 1\n",
    "# h' is not affcted from Abs(v1)\n",
    "constraint_lhs[15, 3] = 1\n",
    "\n",
    "opt = ps.ConstrainedSR3(constraint_lhs=constraint_lhs, constraint_rhs=constraint_rhs, threshold=0.5, thresholder='l1')\n",
    "# opt = ps.SR3(threshold=0.5, thresholder='l1')\n",
    "# library_poly = ps.PolynomialLibrary(degree=2, include_bias=False)\n",
    "library_sym_1 = ps.CustomLibrary(\n",
    "    # library_functions=[lambda x, y, z: abs(y), lambda x, y, z: abs(z)])\n",
    "    library_functions=[lambda h, u, u1, v, v1: abs(u), lambda h, u, u1, v, v1: abs(u1), lambda h, u, u2, v, v1: abs(v),  lambda h, u, u2, v, v1: abs(v1)])\n",
    "combined_library = library_sym_1\n",
    "# feature_names = ['h', 'u', 'v']\n",
    "feature_names = ['h', 'u', 'u1', 'v', 'v1']\n",
    "model_sym_1 = ps.SINDy(feature_library = combined_library, feature_names = feature_names, optimizer=opt)\n",
    "\n",
    "\n",
    "model_sym_1.fit(x_train, t=T_train, multiple_trajectories=True)\n",
    "model_sym_1.print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute L2 error 0.004802322289611073\n",
      "relative L2 error 0.04004687704426924\n",
      "mean 0.11991752276469438\n",
      "None\n",
      "absolute L2 error 0.005634422971122236\n",
      "relative L2 error 0.04465440426340756\n",
      "mean 0.12617843780617655\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(compute_error(model_sym_1, x_test , T_test))\n",
    "print(compute_error(model_sym_1, x_train, T_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- [x] check why there are so little non-zeros. Something wrong in callback.h?\n",
    "- [ ] generate mid/high resulition data\n",
    "- [ ] use only newtonian model, not chezy\n",
    "- [x] use level 0 system.\n",
    "- [ ] Ideally, I do not want to recompute the moment projection of the high fidality data every time. Can I just use the first N-levels of a given one? I think this requires changes in `callback.h` and fit the Q fields to the correct size and PICK THE RIGHT FIELDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
